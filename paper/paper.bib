@article{numpy,
  title={Array programming with {NumPy}},
  author={Harris, Charles R and Millman, K Jarrod and van der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{numpy,
  title={Array programming with {NumPy}},
  author={Harris, Charles R and Millman, K Jarrod and van der Walt, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J and others},
  journal={Nature},
  volume={585},
  number={7825},
  pages={357--362},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{scikitlearn,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'E}douard},
  journal={Journal of Machine Learning Research},
  volume={12},
  pages={2825--2830},
  year={2011}
}

@inproceedings{pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Alexander and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle={Advances in Neural Information Processing Systems 32},
  editor={H. Wallach and H. Larochelle and A. Beygelzimer and F. d'Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages={8024--8035},
  year={2019},
  publisher={Curran Associates, Inc.},
  url={http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{h5py,
  author = {Andrew Collette},
  title = {h5py: HDF5 for Python},
  year = {2023},
  howpublished = {\url{http://www.h5py.org}},
}


@inproceedings{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640366},
doi = {10.1145/3620665.3640366},
abstract = {This paper introduces two extensions to the popular PyTorch machine learning framework, TorchDynamo and TorchInductor, which implement the torch.compile feature released in PyTorch 2. TorchDynamo is a Python-level just-in-time (JIT) compiler that enables graph compilation in PyTorch programs without sacrificing the flexibility of Python. It achieves this by dynamically modifying Python bytecode before execution and extracting sequences of PyTorch operations into an FX graph, which is then JIT compiled using one of many extensible backends. TorchInductor is the default compiler backend for TorchDynamo, which translates PyTorch programs into OpenAI's Triton for GPUs and C++ for CPUs. Results show that TorchDynamo is able to capture graphs more robustly than prior approaches while adding minimal overhead, and TorchInductor is able to provide a 2.27\texttimes{} inference and 1.41\texttimes{} training geometric mean speedup on an NVIDIA A100 GPU across 180+ real-world models, which outperforms six other compilers. These extensions provide a new way to apply optimizations through compilers in eager mode frameworks like PyTorch.},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {929â€“947},
numpages = {19},
location = {, La Jolla, CA, USA, },
series = {ASPLOS '24}
}


@phdthesis{brando2022thesis,
  author       = {A. Brando},
  title        = {Aleatoric uncertainty modelling in regression problems using deep learning},
  school       = {Universitat de Barcelona},
  year         = {2022}
}

@ARTICLE{Tran2019arXiv191210066T,
       author = {{Tran}, Kevin and {Neiswanger}, Willie and {Yoon}, Junwoong and {Zhang}, Qingyang and {Xing}, Eric and {Ulissi}, Zachary W.},
        title = "{Methods for comparing uncertainty quantifications for material property predictions}",
      journal = {arXiv e-prints},
     keywords = {Condensed Matter - Materials Science, Physics - Computational Physics},
         year = 2019,
        month = dec,
          eid = {arXiv:1912.10066},
        pages = {arXiv:1912.10066},
          doi = {10.48550/arXiv.1912.10066},
archivePrefix = {arXiv},
       eprint = {1912.10066},
 primaryClass = {cond-mat.mtrl-sci},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191210066T},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Gal2022NatRP...4..573G,
       author = {{Gal}, Yarin and {Koumoutsakos}, Petros and {Lanusse}, Francois and {Louppe}, Gilles and {Papadimitriou}, Costas},
        title = "{Bayesian uncertainty quantification for machine-learned models in physics}",
      journal = {Nature Reviews Physics},
         year = 2022,
        month = aug,
       volume = {4},
       number = {9},
        pages = {573-577},
          doi = {10.1038/s42254-022-00498-4},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022NatRP...4..573G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}



@ARTICLE{GalMCDropout2015arXiv150602142G,
       author = {{Gal}, Yarin and {Ghahramani}, Zoubin},
        title = "{Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2015,
        month = jun,
          eid = {arXiv:1506.02142},
        pages = {arXiv:1506.02142},
          doi = {10.48550/arXiv.1506.02142},
archivePrefix = {arXiv},
       eprint = {1506.02142},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150602142G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Caldeira2020arXiv200410710C,
       author = {{Caldeira}, Jo{\~a}o and {Nord}, Brian},
        title = "{Deeply Uncertain: Comparing Methods of Uncertainty Quantification in Deep Learning Algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
         year = 2020,
        month = apr,
          eid = {arXiv:2004.10710},
        pages = {arXiv:2004.10710},
          doi = {10.48550/arXiv.2004.10710},
archivePrefix = {arXiv},
       eprint = {2004.10710},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200410710C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@article{Yang2023,
author = {Yang, Chu-I and Li, Yi-Pei},
year = {2023},
month = {02},
pages = {},
title = {Explainable uncertainty quantifications for deep learning-based molecular property prediction},
volume = {15},
journal = {Journal of Cheminformatics},
doi = {10.1186/s13321-023-00682-3}
}

@ARTICLE{Seitzer2022arXiv220309168S,
       author = {{Seitzer}, Maximilian and {Tavakoli}, Arash and {Antic}, Dimitrije and {Martius}, Georg},
        title = "{On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2022,
        month = mar,
          eid = {arXiv:2203.09168},
        pages = {arXiv:2203.09168},
          doi = {10.48550/arXiv.2203.09168},
archivePrefix = {arXiv},
       eprint = {2203.09168},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220309168S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{Kendall2017arXiv170304977K,
       author = {{Kendall}, Alex and {Gal}, Yarin},
        title = "{What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Vision and Pattern Recognition},
         year = 2017,
        month = mar,
          eid = {arXiv:1703.04977},
        pages = {arXiv:1703.04977},
          doi = {10.48550/arXiv.1703.04977},
archivePrefix = {arXiv},
       eprint = {1703.04977},
 primaryClass = {cs.CV},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170304977K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@INPROCEEDINGS{Nix374138,
  author={Nix, D.A. and Weigend, A.S.},
  booktitle={Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)}, 
  title={Estimating the mean and variance of the target probability distribution}, 
  year={1994},
  volume={1},
  number={},
  pages={55-60 vol.1},
  keywords={Probability distribution;Noise level;Feedforward systems;Computer science;Cognitive science;Computer errors;Measurement uncertainty;Cost function;Equations;Error correction},
  doi={10.1109/ICNN.1994.374138}}

@ARTICLE{Lakshminarayanan2016arXiv161201474L,
       author = {{Lakshminarayanan}, Balaji and {Pritzel}, Alexander and {Blundell}, Charles},
        title = "{Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = 2016,
        month = dec,
          eid = {arXiv:1612.01474},
        pages = {arXiv:1612.01474},
          doi = {10.48550/arXiv.1612.01474},
archivePrefix = {arXiv},
       eprint = {1612.01474},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161201474L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{brando2023standardizing,
  title={Standardizing the probabilistic sources of uncertainty for the sake of safety deep learning},
  author={Brando, Axel and Serra, Isabel and Mezzetti, Enrico and Cazorla Almeida, Francisco Javier and Abella Ferrer, Jaume},
  booktitle={Proceedings of the Workshop on Artificial Intelligence Safety 2023 (SafeAI 2023) co-located with the Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI 2023): Washington DC, USA, February 13-14, 2023.},
  volume={3381},
  year={2023},
  organization={CEUR Workshop Proceedings}
}

@ARTICLE{Seitzer2022,
       author = {{Seitzer}, Maximilian and {Tavakoli}, Arash and {Antic}, Dimitrije and {Martius}, Georg},
        title = "{On the Pitfalls of Heteroscedastic Uncertainty Estimation with Probabilistic Neural Networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = 2022,
        month = mar,
          eid = {arXiv:2203.09168},
        pages = {arXiv:2203.09168},
          doi = {10.48550/arXiv.2203.09168},
archivePrefix = {arXiv},
       eprint = {2203.09168},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220309168S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{Meinert2022DER,
       author = {{Meinert}, Nis and {Gawlikowski}, Jakob and {Lavin}, Alexander},
        title = "{The Unreasonable Effectiveness of Deep Evidential Regression}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2022,
        month = may,
          eid = {arXiv:2205.10060},
        pages = {arXiv:2205.10060},
          doi = {10.48550/arXiv.2205.10060},
archivePrefix = {arXiv},
       eprint = {2205.10060},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220510060M},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Amini2019DER,
       author = {{Amini}, Alexander and {Schwarting}, Wilko and {Soleimany}, Ava and {Rus}, Daniela},
        title = "{Deep Evidential Regression}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
         year = 2019,
        month = oct,
          eid = {arXiv:1910.02600},
        pages = {arXiv:1910.02600},
          doi = {10.48550/arXiv.1910.02600},
archivePrefix = {arXiv},
       eprint = {1910.02600},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191002600A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{PS2017,
author = {Nicholas G. Polson and Vadim Sokolov},
title = {{Deep Learning: A Bayesian Perspective}},
volume = {12},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {1275 -- 1304},
keywords = {Artificial intelligence, Bayesian hierarchical models, deep learning, LSTM models, machine learning, pattern matching, prediction, TensorFlow},
year = {2017},
doi = {10.1214/17-BA1082},
URL = {https://doi.org/10.1214/17-BA1082}
}

@INPROCEEDINGS{LV2000,
  author={Lampinen, Jouko and Vehtari, Aki},
  booktitle={2000 10th European Signal Processing Conference}, 
  title={Bayesian techniques for neural networks â€” Review and case studies}, 
  year={2000},
  volume={},
  number={},
  pages={1-8},
  keywords={Bayes methods;Noise;Computational modeling;Data models;Complexity theory;Predictive models;Neural networks},
  doi={}}

@article{T2004,
author = {D. M. Titterington},
title = {{Bayesian Methods for Neural Networks and Related Models}},
volume = {19},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {128 -- 139},
keywords = {Bayesian methods, Bayesian model choice, feed-forward neural network, Graphical model, Laplace approximation, machine learning, Markov chain Monte Carlo, variational approximation},
year = {2004},
doi = {10.1214/088342304000000099},
URL = {https://doi.org/10.1214/088342304000000099}
}

@book{ZZ2012, 
author = {Zhou, Zhi-Hua}, title = {Ensemble Methods: Foundations and Algorithms}, year = {2012}, isbn = {1439830037}, publisher = {Chapman \& Hall/CRC}, edition = {1st}, abstract = {An up-to-date, self-contained introduction to a state-of-the-art machine learning approach, Ensemble Methods: Foundations and Algorithms shows how these accurate methods are used in real-world tasks. It gives you the necessary groundwork to carry out further research in this evolving field. After presenting background and terminology, the book covers the main algorithms and theories, including Boosting, Bagging, Random Forest, averaging and voting schemes, the Stacking method, mixture of experts, and diversity measures. It also discusses multiclass extension, noise tolerance, error-ambiguity and bias-variance decompositions, and recent progress in information theoretic diversity. Moving on to more advanced topics, the author explains how to achieve better performance through ensemble pruning and how to generate better clustering results by combining multiple clusterings. In addition, he describes developments of ensemble methods in semi-supervised learning, active learning, cost-sensitive learning, class-imbalance learning, and comprehensibility enhancement.} }

